{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2kfxuvLXOzp"
      },
      "source": [
        "# Lab 9 - Introduction to Pytorch and Metrics Tracking\n",
        "\n",
        "- **Author:** Satej Soman\n",
        "- **Date:** April 2, 2025\n",
        "- **Course:** INFO 251: Applied Machine Learning\n",
        "\n",
        "## Topics\n",
        "1. Python neural networks libraries\n",
        "2. Computational substrates\n",
        "3. Typical training setup\n",
        "4. Metrics tracking\n",
        "\n",
        "## Goals\n",
        "By the end of this lab, you should be able to:\n",
        "1. define a neural network in Python\n",
        "2. train a neural network on a cloud-hosted compute instance\n",
        "3. track key metrics during neural network training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "stksizJ4XOzq"
      },
      "outputs": [],
      "source": [
        "%pip install torch torchvision numpy scikit-learn matplotlib tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V8M4iqcXOzr"
      },
      "outputs": [],
      "source": [
        "# old friends\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# and new\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tensorboard import summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# analogy:\n",
        "\n",
        "\n",
        "$(X'X)^{-1}(X'y)$  <->  `sklearn.linear_model.LinearRegression`\n",
        "\n",
        "$\\beta_t = \\beta_{t-1} - R \\nabla_\\beta J$ <-> neural network libraries"
      ],
      "metadata": {
        "id": "NCIyJ-JE_lnz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lx6S9vp1XOzs"
      },
      "outputs": [],
      "source": [
        "# let's generate some data\n",
        "\n",
        "X = np.hstack([np.ones((100, 1)), np.random.uniform(0, 10, size = (100, 2))])\n",
        "y = X @ np.array([-1, 2, -2])\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize = (10, 4))\n",
        "for (d, ax) in enumerate(axs, start=1):\n",
        "    plt.sca(ax)\n",
        "    plt.scatter(X[:, d], y)\n",
        "    plt.title(f\"synthetic regression data - dimension {d}\")\n",
        "    plt.xlabel(f\"$X_{d}$\")\n",
        "    plt.ylabel(\"$y$\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# analytical solution:\n",
        "\n",
        "def linear_regression_analytical(X, y):\n",
        "    return np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "\n",
        "regression_coefficients_analytical = linear_regression_analytical(X, y)\n",
        "print(f\"analytical regression coefficients: {regression_coefficients_analytical}\")\n",
        "\n",
        "linreg = LinearRegression(fit_intercept=False).fit(X, y)\n",
        "print(\"sklearn regression coefficients:\", linreg.coef_)\n"
      ],
      "metadata": {
        "id": "8guZxjAOcLue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# numerical solution:\n",
        "\n",
        "def linear_regression_gradient_descent(X, y, R=0.01, max_iter=5000, tol=1e-6):\n",
        "    (n, d) = X.shape\n",
        "    Δβ, βt = float(\"inf\"), [np.zeros(d)]\n",
        "    while len(βt) < max_iter and np.max(np.abs(Δβ)) > tol:\n",
        "        Δβ = -R/n * X.T @ (X @ βt[-1] - y)\n",
        "        βt.append(βt[-1] + Δβ)\n",
        "    return βt\n",
        "\n",
        "gradient_descent_iterates = linear_regression_gradient_descent(X, y)\n",
        "print(f\"gradient descent regression coefficients: {gradient_descent_iterates[-1]}\")"
      ],
      "metadata": {
        "id": "lNUce68Kgiaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pytorch equivalent\n",
        "\n",
        "- loop taken from the [pytorch docs](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)\n",
        "\n",
        "- higher-level libraries (e.g. pytorch lightning) optimize this away"
      ],
      "metadata": {
        "id": "uRtqy7GSCqwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configure training objective\n",
        "\n",
        "class PytorchLinearRegression(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(d, 1, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(torch.tensor(x, dtype=torch.float32))\n",
        "\n",
        "model = PytorchLinearRegression(d=2)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# core training loop\n",
        "num_epochs = 5000\n",
        "for i in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(X[:, 1:])\n",
        "    loss_value = loss(y_pred, torch.tensor(y, dtype=torch.float32).reshape(-1, 1))\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"pytorch coefficients:\", np.array([*model.linear.bias.data.numpy(), *model.linear.weight.data.numpy().flatten()]))\n",
        "\n",
        "# report training loss\n",
        "print(f\"training loss: {loss_value.item()}\")\n"
      ],
      "metadata": {
        "id": "fNdB_fHFCt7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# a more realistic pytorch example\n",
        "\n",
        "(again following the [pytorch docs](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html))\n",
        "\n",
        "let's build a classifier to classify images from the [Fashion-MNIST dataset](https://paperswithcode.com/dataset/fashion-mnist)."
      ],
      "metadata": {
        "id": "QCfIGQOOGGjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# common image processing step: specify how we want our inputs to be passed\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# just like sklearn, DL libraries have utilities for common datasets\n",
        "trn_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
        "val_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# Create data loaders for our datasets; shuffle for training, not for validation\n",
        "BATCH_SIZE = 4\n",
        "trn_loader = torch.utils.data.DataLoader(trn_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Class labels\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
        "\n",
        "# Report split sizes\n",
        "print()\n",
        "print('Training set has {} instances'.format(len(trn_set)))\n",
        "print('Validation set has {} instances'.format(len(val_set)))"
      ],
      "metadata": {
        "id": "uyG2E72lGFjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "viz_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "viz_loader = torch.utils.data.DataLoader(viz_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "batch = (batch_imgs, batch_labels) = next(iter(viz_loader)) # get a batch of images\n",
        "batch\n"
      ],
      "metadata": {
        "id": "CJjSkXThH48H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, BATCH_SIZE, figsize = (10, 4))\n",
        "for (i, ax) in enumerate(axs):\n",
        "    ax.imshow(np.asarray(batch_imgs[i].squeeze()), cmap='Greys')\n",
        "    label_index = batch_labels[i].item()\n",
        "    label = classes[label_index]\n",
        "    ax.set_xticks([1], [f\"label: {label}\"])\n",
        "    ax.set_yticks([])\n"
      ],
      "metadata": {
        "id": "P0RVCd4BIbC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## let's define our model"
      ],
      "metadata": {
        "id": "nzlPRPsvOY-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.activation = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Sc_osuiyOhuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what do we do in a single epoch?\n",
        "\n",
        "def train_epoch(model, loss_fn, optimizer, trn_loader, device=torch.device(\"cpu\")):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    epoch_start = time.time()\n",
        "    start = time.time()\n",
        "    for i, data in enumerate(trn_loader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:\n",
        "            last_loss = running_loss / 1000 # loss per batch\n",
        "            end = time.time()\n",
        "            batch_time = round(end - start, 2)\n",
        "            print('  batch {} - loss: {}, time taken for 1k batches: {}s'.format(i + 1, last_loss, batch_time))\n",
        "            running_loss = 0.\n",
        "            start = time.time()\n",
        "    epoch_end = time.time()\n",
        "    epoch_time = round(epoch_end - epoch_start, 2)\n",
        "    print(f\"epoch time: {epoch_time}s\")\n",
        "    return last_loss"
      ],
      "metadata": {
        "id": "MGLD5YaaRkoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# entire training loop\n",
        "\n",
        "def train_model(model, loss_fn, optimizer, trn_loader, val_loader, num_epochs, device=torch.device(\"cpu\")):\n",
        "    model = model.to(device)\n",
        "    loss_fn = loss_fn.to(device)\n",
        "\n",
        "    best_vloss = float(\"inf\")\n",
        "    best_epoch = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('EPOCH {}:'.format(epoch + 1))\n",
        "\n",
        "        # Make sure gradient tracking is on, and do a pass over the data\n",
        "        model.train(True)\n",
        "        avg_loss = train_epoch(model, loss_fn, optimizer, trn_loader, device=device)\n",
        "\n",
        "        running_vloss = 0.0\n",
        "        # Set the model to evaluation mode, disabling dropout and using population\n",
        "        # statistics for batch normalization.\n",
        "        model.eval()\n",
        "\n",
        "        # Disable gradient computation and reduce memory consumption.\n",
        "        with torch.no_grad():\n",
        "            for i, vdata in enumerate(val_loader):\n",
        "                vinputs, vlabels = vdata\n",
        "                vinputs = vinputs.to(device)\n",
        "                vlabels = vlabels.to(device)\n",
        "                voutputs = model(vinputs)\n",
        "                vloss = loss_fn(voutputs, vlabels)\n",
        "                running_vloss += vloss\n",
        "\n",
        "        avg_vloss = running_vloss / (i + 1)\n",
        "        print('LOSS: trn {} val {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "        # Track best performance, and save the model's state\n",
        "        if avg_vloss < best_vloss:\n",
        "            best_vloss = avg_vloss\n",
        "            best_epoch = epoch\n"
      ],
      "metadata": {
        "id": "36sC3L4FRpVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = MNISTClassifier(len(classes))\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "num_epochs = 5\n",
        "train_model(classifier, loss_fn, optimizer, trn_loader, val_loader, num_epochs)"
      ],
      "metadata": {
        "id": "mTy30ol2bJNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training on a GPU instead of a CPU"
      ],
      "metadata": {
        "id": "lZLSnYTWHcE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU availability\n",
        "print(torch.cuda.is_available())\n",
        "gpu = torch.device(\"cuda\")\n",
        "\n",
        "# Create data loaders for our datasets; shuffle for training, not for validation\n",
        "BATCH_SIZE = 4\n",
        "trn_loader = torch.utils.data.DataLoader(trn_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "classifier = MNISTClassifier(len(classes))\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "train_model(classifier, loss_fn, optimizer, trn_loader, val_loader, num_epochs, device=gpu)"
      ],
      "metadata": {
        "id": "1fHWpzGMHezE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "trn_loader = torch.utils.data.DataLoader(trn_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "classifier = MNISTClassifier(len(classes))\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "train_model(classifier, loss_fn, optimizer, trn_loader, val_loader, num_epochs, device=gpu)"
      ],
      "metadata": {
        "id": "X2LX2xdybha3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tracking metrics"
      ],
      "metadata": {
        "id": "Ypvl_jpKAmdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see how our gradient descent estimates changed over time\n",
        "\n",
        "for i in (0, 1, 2):\n",
        "    plt.scatter(\n",
        "        list(range(len(gradient_descent_iterates))),\n",
        "        [β[i] for β in gradient_descent_iterates],\n",
        "        label = f\"β$_{i}(t)$\", s = 1)\n",
        "plt.legend()\n",
        "plt.title(\"gradient descent iterates vs number of steps\")\n",
        "plt.xlabel(\"$t$\")\n",
        "plt.ylabel(\"β\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lxzd1TxzAo9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "some fancier tracking options:\n",
        "- TensorBoard ([example with Keras](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_in_notebooks.ipynb))\n",
        "\n",
        "- Weights and Biases (wandb.ai)\n"
      ],
      "metadata": {
        "id": "M3Ryw0yrhSxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Load the TensorBoard notebook extension (only once per notebook)\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Start TensorBoard\n",
        "%tensorboard --logdir runs\n",
        "\n",
        "def train_model(model, loss_fn, optimizer, trn_loader, val_loader, num_epochs, device=torch.device(\"cpu\")):\n",
        "    model = model.to(device)\n",
        "    loss_fn = loss_fn.to(device)\n",
        "\n",
        "    writer = SummaryWriter()  # Initialize SummaryWriter\n",
        "\n",
        "    best_vloss = float(\"inf\")\n",
        "    best_epoch = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('EPOCH {}:'.format(epoch + 1))\n",
        "\n",
        "        model.train(True)\n",
        "        avg_loss = train_epoch(model, loss_fn, optimizer, trn_loader, device=device)\n",
        "        writer.add_scalar('Loss/train', avg_loss, epoch) # Log training loss\n",
        "\n",
        "        running_vloss = 0.0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, vdata in enumerate(val_loader):\n",
        "                vinputs, vlabels = vdata\n",
        "                vinputs = vinputs.to(device)\n",
        "                vlabels = vlabels.to(device)\n",
        "                voutputs = model(vinputs)\n",
        "                vloss = loss_fn(voutputs, vlabels)\n",
        "                running_vloss += vloss\n",
        "\n",
        "        avg_vloss = running_vloss / (i + 1)\n",
        "        print('LOSS: trn {} val {}'.format(avg_loss, avg_vloss))\n",
        "        writer.add_scalar('Loss/validation', avg_vloss, epoch) # Log validation loss\n",
        "        writer.flush()\n",
        "\n",
        "        if avg_vloss < best_vloss:\n",
        "            best_vloss = avg_vloss\n",
        "            best_epoch = epoch\n",
        "\n",
        "    writer.close() # Close the writer when done\n",
        "    print(f\"best epoch: {best_epoch}\")\n",
        "classifier = MNISTClassifier(len(classes))\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "num_epochs = 5\n",
        "train_model(classifier, loss_fn, optimizer, trn_loader, val_loader, num_epochs, device=gpu)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HedtWegiAsiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "other metrics to track:\n",
        "\n",
        "- prediction metrics\n",
        "    - classification\n",
        "        - accuracy\n",
        "        - recall\n",
        "        - F1\n",
        "        - ROC-AUC\n",
        "    - regression\n",
        "        - R2\n",
        "\n",
        "- model internals\n",
        "    - gradient magnitudes\n",
        "    - parameter values\n",
        "    - maximally-activating instances for each layer"
      ],
      "metadata": {
        "id": "OInHSYL-hJGM"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}